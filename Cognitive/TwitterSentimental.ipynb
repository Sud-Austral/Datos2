{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "{'positive': 0.21, 'neutral': 0.74, 'negative': 0.05}\n",
      "Document Sentiment: neutral\n",
      "Overall scores: positive=0.21; neutral=0.74; negative=0.05 \n",
      "\n",
      "[Length: 27]\n",
      "Sentence 1 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.44\n",
      "Neutral=0.52\n",
      "Negative=0.04\n",
      "\n",
      "[Length: 38]\n",
      "Sentence 2 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.11\n",
      "Neutral=0.85\n",
      "Negative=0.04\n",
      "\n",
      "[Length: 72]\n",
      "Sentence 3 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.09\n",
      "Neutral=0.84\n",
      "Negative=0.07\n",
      "\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "key = \"3f6e681ce10c430fadc5fa12b5899774\"\n",
    "endpoint = \"https://tweetstopowerbi8.cognitiveservices.azure.com/\"\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "def sentiment_analysis_example(client):\n",
    "    #documents = [\"Estoy feliz\",\"Estoy feliz\",\"Estoy feliz\",\"Estoy feliz\",\"Estoy feliz\",\"Estoy feliz\"]\n",
    "    documents = list(data.Contenido)\n",
    "    #print(len(documents))\n",
    "    print(len(client.analyze_sentiment(documents = documents, language=\"es\")))\n",
    "    response = client.analyze_sentiment(documents = documents, language=\"es\")[0]\n",
    "    #response = client.analyze_sentiment(documents = documents)[0]\n",
    "    print(response.confidence_scores)\n",
    "    print(\"Document Sentiment: {}\".format(response.sentiment))\n",
    "    print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "        response.confidence_scores.positive,\n",
    "        response.confidence_scores.neutral,\n",
    "        response.confidence_scores.negative,\n",
    "    ))\n",
    "    for idx, sentence in enumerate(response.sentences):\n",
    "        print(\"[Length: {}]\".format(sentence.grapheme_length))\n",
    "        print(\"Sentence {} sentiment: {}\".format(idx+1, sentence.sentiment))\n",
    "        print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "            sentence.confidence_scores.positive,\n",
    "            sentence.confidence_scores.neutral,\n",
    "            sentence.confidence_scores.negative,\n",
    "        ))\n",
    "    \"\"\"\n",
    "    response = client.detect_language(documents = documents, country_hint = 'us')[0]\n",
    "    print(\"Language: \", response.primary_language.name)\n",
    "    \n",
    "    response = client.extract_key_phrases(documents = documents)[0]\n",
    "    for phrase in response.key_phrases:\n",
    "        print(\"\\t\\t\", phrase)\n",
    "    \"\"\"\n",
    "    print(\"***************************************************************************\")    \n",
    "    \n",
    "    \n",
    "sentiment_analysis_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documents': [{'detectedLanguages': [{'iso6391Name': 'en',\n",
      "                                       'name': 'English',\n",
      "                                       'score': 1.0}],\n",
      "                'id': '1'},\n",
      "               {'detectedLanguages': [{'iso6391Name': 'es',\n",
      "                                       'name': 'Spanish',\n",
      "                                       'score': 1.0}],\n",
      "                'id': '2'},\n",
      "               {'detectedLanguages': [{'iso6391Name': 'zh_chs',\n",
      "                                       'name': 'Chinese_Simplified',\n",
      "                                       'score': 1.0}],\n",
      "                'id': '3'}],\n",
      " 'errors': []}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# pprint is used to format the JSON response\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "subscription_key = \"3f6e681ce10c430fadc5fa12b5899774\"\n",
    "endpoint = \"https://tweetstopowerbi8.cognitiveservices.azure.com/\"\n",
    "\n",
    "language_api_url = endpoint + \"text/analytics/v2.1/languages\"\n",
    "\n",
    "documents = {\"documents\": [\n",
    "    {\"id\": \"1\", \"text\": \"This is a document written in English.\"},\n",
    "    {\"id\": \"2\", \"text\": \"Este es un document escrito en Español.\"},\n",
    "    {\"id\": \"3\", \"text\": \"这是一个用中文写的文件\"}\n",
    "]}\n",
    "\n",
    "headers = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "response = requests.post(language_api_url, headers=headers, json=documents)\n",
    "languages = response.json()\n",
    "pprint(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_url = endpoint + \"/text/analytics/v2.1/sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documents': [{'id': '1', 'score': 0.9708490371704102},\n",
      "               {'id': '2', 'score': 0.0019068121910095215},\n",
      "               {'id': '3', 'score': 0.7456425428390503},\n",
      "               {'id': '4', 'score': 0.9528752565383911}],\n",
      " 'errors': []}\n"
     ]
    }
   ],
   "source": [
    "documents = {\"documents\": [\n",
    "    {\"id\": \"1\", \"language\": \"en\",\n",
    "        \"text\": \"I had a wonderful experience! The rooms were wonderful and the staff was helpful.\"},\n",
    "    {\"id\": \"2\", \"language\": \"en\",\n",
    "        \"text\": \"I had a terrible time at the hotel. The staff was rude and the food was awful.\"},\n",
    "    {\"id\": \"3\", \"language\": \"es\",\n",
    "        \"text\": \"Los caminos que llevan hasta Monte Rainier son espectaculares y hermosos.\"},\n",
    "    {\"id\": \"4\", \"language\": \"es\",\n",
    "     \"text\": \"Estoy feliz\"}\n",
    "]}\n",
    "\n",
    "headers = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "response = requests.post(sentiment_url, headers=headers, json=documents)\n",
    "sentiments = response.json()\n",
    "pprint(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"../Twitter/Tweet.csv\")\n",
    "type(list(data.Contenido))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
